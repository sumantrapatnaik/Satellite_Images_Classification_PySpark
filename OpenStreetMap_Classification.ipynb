{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Sumantra Patnaik\n",
    "\n",
    "# Multi-Class classification using Apache Spark MLLib\n",
    "\n",
    "# Dataset : Crowdsourced data from OpenStreetMap - used to automate the classification of satellite images \n",
    "# into different land cover classes (impervious, farm, forest, grass, orchard, water)\n",
    "\n",
    "# Algorithms Used: Multinomial Logistic Regression, Decision Trees, Random Forest\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('OpenStreetMap_Classification').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.csv(\"OpenStreetMap_Data/training.csv\", header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(\"OpenStreetMap_Data/testing.csv\", header=True, inferSchema=True)\n",
    "comb_df = train_df.union(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- max_ndvi: double (nullable = true)\n",
      " |-- 20150720_N: double (nullable = true)\n",
      " |-- 20150602_N: double (nullable = true)\n",
      " |-- 20150517_N: double (nullable = true)\n",
      " |-- 20150501_N: double (nullable = true)\n",
      " |-- 20150415_N: double (nullable = true)\n",
      " |-- 20150330_N: double (nullable = true)\n",
      " |-- 20150314_N: double (nullable = true)\n",
      " |-- 20150226_N: double (nullable = true)\n",
      " |-- 20150210_N: double (nullable = true)\n",
      " |-- 20150125_N: double (nullable = true)\n",
      " |-- 20150109_N: double (nullable = true)\n",
      " |-- 20141117_N: double (nullable = true)\n",
      " |-- 20141101_N: double (nullable = true)\n",
      " |-- 20141016_N: double (nullable = true)\n",
      " |-- 20140930_N: double (nullable = true)\n",
      " |-- 20140813_N: double (nullable = true)\n",
      " |-- 20140626_N: double (nullable = true)\n",
      " |-- 20140610_N: double (nullable = true)\n",
      " |-- 20140525_N: double (nullable = true)\n",
      " |-- 20140509_N: double (nullable = true)\n",
      " |-- 20140423_N: double (nullable = true)\n",
      " |-- 20140407_N: double (nullable = true)\n",
      " |-- 20140322_N: double (nullable = true)\n",
      " |-- 20140218_N: double (nullable = true)\n",
      " |-- 20140202_N: double (nullable = true)\n",
      " |-- 20140117_N: double (nullable = true)\n",
      " |-- 20140101_N: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'max_ndvi',\n",
       " '20150720_N',\n",
       " '20150602_N',\n",
       " '20150517_N',\n",
       " '20150501_N',\n",
       " '20150415_N',\n",
       " '20150330_N',\n",
       " '20150314_N',\n",
       " '20150226_N',\n",
       " '20150210_N',\n",
       " '20150125_N',\n",
       " '20150109_N',\n",
       " '20141117_N',\n",
       " '20141101_N',\n",
       " '20141016_N',\n",
       " '20140930_N',\n",
       " '20140813_N',\n",
       " '20140626_N',\n",
       " '20140610_N',\n",
       " '20140525_N',\n",
       " '20140509_N',\n",
       " '20140423_N',\n",
       " '20140407_N',\n",
       " '20140322_N',\n",
       " '20140218_N',\n",
       " '20140202_N',\n",
       " '20140117_N',\n",
       " '20140101_N']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     class|\n",
      "+----------+\n",
      "|      farm|\n",
      "|     grass|\n",
      "|     water|\n",
      "|impervious|\n",
      "|    forest|\n",
      "|   orchard|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Target classes to classify\n",
    "comb_df.select('class').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_cols = comb_df.select(['max_ndvi','20150720_N','20150602_N','20150517_N','20150501_N','20150415_N','20150330_N','20150314_N',\n",
    "                          '20150226_N','20150210_N','20150125_N','20150109_N','20141117_N','20141101_N','20141016_N',\n",
    "                           '20140930_N','20140813_N','20140626_N','20140610_N','20140525_N','20140509_N','20140423_N',\n",
    "                           '20140407_N','20140322_N','20140218_N','20140202_N','20140117_N','20140101_N'])\n",
    "feature_cols = ['max_ndvi','20150720_N','20150602_N','20150517_N','20150501_N','20150415_N','20150330_N','20150314_N',\n",
    "                          '20150226_N','20150210_N','20150125_N','20150109_N','20141117_N','20141101_N','20141016_N',\n",
    "                           '20140930_N','20140813_N','20140626_N','20140610_N','20140525_N','20140509_N','20140423_N',\n",
    "                           '20140407_N','20140322_N','20140218_N','20140202_N','20140117_N','20140101_N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0\n",
      "max_ndvi 0\n",
      "20150720_N 0\n",
      "20150602_N 0\n",
      "20150517_N 0\n",
      "20150501_N 0\n",
      "20150415_N 0\n",
      "20150330_N 0\n",
      "20150314_N 0\n",
      "20150226_N 0\n",
      "20150210_N 0\n",
      "20150125_N 0\n",
      "20150109_N 0\n",
      "20141117_N 0\n",
      "20141101_N 0\n",
      "20141016_N 0\n",
      "20140930_N 0\n",
      "20140813_N 0\n",
      "20140626_N 0\n",
      "20140610_N 0\n",
      "20140525_N 0\n",
      "20140509_N 0\n",
      "20140423_N 0\n",
      "20140407_N 0\n",
      "20140322_N 0\n",
      "20140218_N 0\n",
      "20140202_N 0\n",
      "20140117_N 0\n",
      "20140101_N 0\n"
     ]
    }
   ],
   "source": [
    "#Find columns havin missing values in them\n",
    "for column in comb_df.columns:\n",
    "    print (column,comb_df.filter(comb_df[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,StringIndexer,OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_indexer = StringIndexer(inputCol = 'class',outputCol = 'TargetClass')\n",
    "model = class_indexer.fit(comb_df)\n",
    "comb_df_1 = model.transform(comb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- max_ndvi: double (nullable = true)\n",
      " |-- 20150720_N: double (nullable = true)\n",
      " |-- 20150602_N: double (nullable = true)\n",
      " |-- 20150517_N: double (nullable = true)\n",
      " |-- 20150501_N: double (nullable = true)\n",
      " |-- 20150415_N: double (nullable = true)\n",
      " |-- 20150330_N: double (nullable = true)\n",
      " |-- 20150314_N: double (nullable = true)\n",
      " |-- 20150226_N: double (nullable = true)\n",
      " |-- 20150210_N: double (nullable = true)\n",
      " |-- 20150125_N: double (nullable = true)\n",
      " |-- 20150109_N: double (nullable = true)\n",
      " |-- 20141117_N: double (nullable = true)\n",
      " |-- 20141101_N: double (nullable = true)\n",
      " |-- 20141016_N: double (nullable = true)\n",
      " |-- 20140930_N: double (nullable = true)\n",
      " |-- 20140813_N: double (nullable = true)\n",
      " |-- 20140626_N: double (nullable = true)\n",
      " |-- 20140610_N: double (nullable = true)\n",
      " |-- 20140525_N: double (nullable = true)\n",
      " |-- 20140509_N: double (nullable = true)\n",
      " |-- 20140423_N: double (nullable = true)\n",
      " |-- 20140407_N: double (nullable = true)\n",
      " |-- 20140322_N: double (nullable = true)\n",
      " |-- 20140218_N: double (nullable = true)\n",
      " |-- 20140202_N: double (nullable = true)\n",
      " |-- 20140117_N: double (nullable = true)\n",
      " |-- 20140101_N: double (nullable = true)\n",
      " |-- TargetClass: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb_df_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = feature_cols,outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = assembler.transform(comb_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(class='water', max_ndvi=997.904, 20150720_N=637.595, 20150602_N=658.668, 20150517_N=-1882.03, 20150501_N=-1924.36, 20150415_N=997.904, 20150330_N=-1739.99, 20150314_N=630.087, 20150226_N=-1628.24, 20150210_N=-1325.64, 20150125_N=-944.084, 20150109_N=277.107, 20141117_N=-206.799, 20141101_N=536.441, 20141016_N=749.348, 20140930_N=-482.993, 20140813_N=492.001, 20140626_N=655.77, 20140610_N=-921.193, 20140525_N=-1043.16, 20140509_N=-1942.49, 20140423_N=267.138, 20140407_N=366.608, 20140322_N=452.238, 20140218_N=211.328, 20140202_N=-2203.02, 20140117_N=-1180.19, 20140101_N=433.906, TargetClass=4.0, features=DenseVector([997.904, 637.595, 658.668, -1882.03, -1924.36, 997.904, -1739.99, 630.087, -1628.24, -1325.64, -944.084, 277.107, -206.799, 536.441, 749.348, -482.993, 492.001, 655.77, -921.193, -1043.16, -1942.49, 267.138, 366.608, 452.238, 211.328, -2203.02, -1180.19, 433.906]))]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data = output.select(['features','TargetClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            features|TargetClass|\n",
      "+--------------------+-----------+\n",
      "|[1522.31,856.499,...|        2.0|\n",
      "|[4288.19,2173.48,...|        2.0|\n",
      "|[955.877,292.614,...|        2.0|\n",
      "|[1165.82,867.414,...|        2.0|\n",
      "|[872.409,174.889,...|        2.0|\n",
      "|[3385.67,3058.11,...|        2.0|\n",
      "|[4202.04,3979.8,2...|        2.0|\n",
      "|[4809.47,4572.18,...|        2.0|\n",
      "|[5686.7,4559.86,5...|        2.0|\n",
      "|[4186.02,3117.1,3...|        2.0|\n",
      "|[1141.28,897.368,...|        2.0|\n",
      "|[1556.6,1253.12,1...|        2.0|\n",
      "|[1664.56,1559.09,...|        2.0|\n",
      "|[6522.16,4189.42,...|        2.0|\n",
      "|[2763.32,2232.54,...|        2.0|\n",
      "|[1156.2,468.962,4...|        2.0|\n",
      "|[1414.92,507.251,...|        2.0|\n",
      "|[1416.28,802.106,...|        2.0|\n",
      "|[4778.76,4778.76,...|        2.0|\n",
      "|[5316.17,4698.36,...|        2.0|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.filter(final_data['TargetClass'] == 2.0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "log_reg_OpenStreetMap = LogisticRegression(featuresCol='features', labelCol='TargetClass', \n",
    "                                           maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model_OpenStreetMap = log_reg_OpenStreetMap.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = lr_model_OpenStreetMap.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|prediction|TargetClass|\n",
      "+----------+-----------+\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        4.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "|       0.0|        2.0|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(['prediction','TargetClass']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error is : 0.307313 \n"
     ]
    }
   ],
   "source": [
    "#Evaluate the Logistic Regression classification model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"TargetClass\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "lr_accuracy = evaluator.evaluate(results)\n",
    "print(\"Test Error is : %g \" % (1.0 - lr_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 for Logistic Regression Classifier is : 0.566928 \n"
     ]
    }
   ],
   "source": [
    "#F1 Score\n",
    "evaluator_F1= MulticlassClassificationEvaluator(\n",
    "    labelCol=\"TargetClass\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "lr_f1_score = dt_evaluator_F1.evaluate(results)\n",
    "print(\"Test F1 for Logistic Regression Classifier is : %g \" % lr_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precison for Logistic Regression Classifier is : 0.479816 \n"
     ]
    }
   ],
   "source": [
    "#weightedPrecision\n",
    "evaluator_Precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"TargetClass\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "lr_precision_score = evaluator_Precision.evaluate(results)\n",
    "print(\"Test Precison for Logistic Regression Classifier is : %g \" % lr_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Recall for Logistic Regression Classifier is : 0.692687 \n"
     ]
    }
   ],
   "source": [
    "#weightedRecall\n",
    "evaluator_Recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"TargetClass\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "lr_recall_score = dt_evaluator_Recall.evaluate(results)\n",
    "print(\"Test Recall for Logistic Regression Classifier is : %g \" % lr_recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decision-Tree classifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"TargetClass\", featuresCol=\"features\")\n",
    "dt_model = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_predictions = dt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+\n",
      "|prediction|TargetClass|            features|\n",
      "+----------+-----------+--------------------+\n",
      "|       2.0|        2.0|[563.444,286.846,...|\n",
      "|       2.0|        2.0|[913.563,776.078,...|\n",
      "|       2.0|        2.0|[959.739,553.443,...|\n",
      "|       2.0|        2.0|[974.948,726.428,...|\n",
      "|       2.0|        2.0|[992.794,481.385,...|\n",
      "+----------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_predictions.select(\"prediction\", \"TargetClass\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for Decision Tree Classifier is : 0.845727 \n",
      "Test Error for Decision Tree Classifier is : 0.154273 \n"
     ]
    }
   ],
   "source": [
    "#Evauate the Decision Tree classifier model\n",
    "#Accuracy\n",
    "dt_accuracy = evaluator_accuracy.evaluate(dt_predictions)\n",
    "print(\"Test Accuracy for Decision Tree Classifier is : %g \" % dt_accuracy)\n",
    "print(\"Test Error for Decision Tree Classifier is : %g \" % (1.0 - dt_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 for Decision Tree Classifier is : 0.836315 \n"
     ]
    }
   ],
   "source": [
    "#F1 Score\n",
    "dt_f1_score = evaluator_F1.evaluate(dt_predictions)\n",
    "print(\"Test F1 for Decision Tree Classifier is : %g \" % dt_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision for Decision Tree Classifier is : 0.836233 \n"
     ]
    }
   ],
   "source": [
    "#Weighted Precision\n",
    "dt_precision_score = evaluator_Precision.evaluate(dt_predictions)\n",
    "print(\"Test Precision for Decision Tree Classifier is : %g \" % dt_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Recall for Decision Tree Classifier is : 0.845727 \n"
     ]
    }
   ],
   "source": [
    "#weighted Recall\n",
    "dt_recall_score = evaluator_Recall.evaluate(dt_predictions)\n",
    "print(\"Test Recall for Decision Tree Classifier is : %g \" % dt_recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random_Forest Classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"TargetClass\", featuresCol=\"features\", numTrees=10)\n",
    "rf_model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict based on the test data\n",
    "rf_predictions = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for Random Forest Classifier is : 0.851898 \n",
      "Test Error for Random Forest Classifier is : 0.148102 \n"
     ]
    }
   ],
   "source": [
    "#Evauate the Random Forest classifier model\n",
    "#Accuracy\n",
    "\n",
    "rf_accuracy = evaluator_accuracy.evaluate(rf_predictions)\n",
    "print(\"Test Accuracy for Random Forest Classifier is : %g \" % rf_accuracy)\n",
    "print(\"Test Error for Random Forest Classifier is : %g \" % (1.0 - rf_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 for Random Forest Classifier is : 0.821483 \n"
     ]
    }
   ],
   "source": [
    "rf_f1_score = evaluator_F1.evaluate(rf_predictions)\n",
    "print(\"Test F1 for Random Forest Classifier is : %g \" % rf_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precison for Random Forest Classifier is : 0.84535 \n"
     ]
    }
   ],
   "source": [
    "rf_precision_score = evaluator_Precision.evaluate(rf_predictions)\n",
    "print(\"Test Precison for Random Forest Classifier is : %g \" % rf_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Recall  for Random Forest Classifier is : 0.851898 \n"
     ]
    }
   ],
   "source": [
    "rf_recall_score = evaluator_Recall.evaluate(rf_predictions)\n",
    "print(\"Test Recall  for Random Forest Classifier is : %g \" % rf_recall_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
